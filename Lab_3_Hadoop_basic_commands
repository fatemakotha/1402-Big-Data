1. Open terminal by right clicking on the desktop icon
2.	Start up Hadoop and the other processess, type:
    start-dfs.sh
    start-yarn.sh
3.	Using the JDK's jps utility to see which Java processes are running, type in the terminal: 
    jps
    
    What is shown? 
    2624 NodeManager
    42259 Jps
    2086 NameNode
    2370 SecondaryNameNode
    2521 ResourceManager
    2209 DataNode
    
    Explain the difference between Local filesystem and Distributed filesystem (see lecture notes)
    A. Local File System (LFS) :
    The basic file system of Linux operating system is termed as Local file system. It stores any data file as it is in single copy.
    It stores data files in Tree format. Here, any user can access data files directly. LFS does not Replicate the data blocks. 
    It always used for storing and processing personal data(small data).

    B. Distributed File System (DFS) :
    When we need to store and process a large data file (approx 1 TB size file at least), the Local file system of Operating system is not appropriate. 
    In such cases we use Distributed File system. 
    It can be created on any Linux operating system with Hadoop. DFS stores any data file by dividing it into several blocks.
    This file system works on Master-Slave format where Master is NameNode and DataNodes are the slaves. 
    All the blocks of a Data file is stored into different DataNodes and the location is only known by NameNode. 
    Every Data Block is replicated into different Datanodes to avoid data loss when any datanode fails. 
    In DFS Data files are directly not accessible to any user because only NameNode knows where the Data blocks of Data file are stored.

4.	Inspect the file system. Then type:
    mkdir -p ~/Data/Books
    What is shown?
    A test.txt file is made in Desktop/~/Data/Books folder
5.	Type:
    echo "To check this out, this is a test. This is only a test." 
    >> ~/Data/Books/test.txt


5.	Type:
    echo "To check this out, this is a test. This is only a test." >> ~/Data/Books/test.txt

6.	Next type:
    cat ~/Data/Books/test.txt 
    What has happened?

7.	Upload your your data to Hadoop using the put command.
    hdfs dfs -put ~/Data/Books/test.txt 
    Check that the data is in the HDFS
    hdfs dfs –ls
    hdfs dfs -cat test.txt
8.	Hadoop comes with a set of demonstration programs. 
    One of them is WordCount.java which will automatically compute the word frequency of all text files found in the HDFS directory you ask it to process.
    The program has several sections:
    •	The Map class takes lines of text that are fed to it (the text files are automatically broken down into lines by Hadoop), 
    and breaks them into words. Outputs a datagram for each word that is a (String, int) tuple, of the form (“some-word", 1), 
    since each tuple corresponds to the first occurrence of each word, so the initial frequency for each word is 1.
    •	The reduce section gets collections of datagrams of the form [( word, n1 ), (word, n2)...] where all the words are the same, 
    ut with different numbers. 
    These collections are the result of a sorting process that is integral to Hadoop and which gathers all the datagrams with the same word together. 
    The reduce process gathers the datagrams inside a data node, and also gathers datagrams from the different data nodes
    into a final collection of datagrams where all the words are now unique, with their total frequency (number of occurrences).
9.	Run the wordcount  and put the results in to a folder called Results:         

    hadoop jar /home/hadoop/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar wordcount test.txt Results
    
    To observe the total execution time (real), include the word time:
    time hadoop jar /home/hadoop/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar wordcount test.txt Results
    
    The time will be shown for real, user and sys, what does this refer to?
    
9.	Run the wordcount  and put the results in to a folder called Results:                                 
hadoop jar /home/hadoop/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar wordcount test.txt Results
To observe the total execution time (real), include the word time:
time hadoop jar /home/hadoop/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar wordcount test.txt Results
The time will be shown for real, user and sys, what does this refer to?

10.	When mapreduce has finished, to get the Results directory type:

    hdfs dfs –get  Results
    Where has the Results directory been moved to?
    
11.	To read the file that has the results, cd to the Results directory and then type in the terminal :
    
    geany part-r-00000

12.	From the terminal window delete test.txt and the Results directory.
    
    hdfs dfs –rmr Results
    hdfs dfs –rm test.txt  
13.	On the desktop go to the Datasets/Lab5 directory. There are a number of books downloaded from the Gutenberg repository:
    The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson		(20417.txt)
    The Notebooks of Leonardo Da Vinci 				(5000.txt)
    The Art of War by 6th cent. B.C. Sunzi 				(132.txt)
    The Adventures of Sherlock Holmes by Sir Arthur Conan Doyle 	(48320.txt)

14.	Copy the files to the Books directory you created earlier

15.	Choose one of the files and load it onto the HDFS

16.	Check that the data has been loaded. 

17.	Run the word count example as before (Hadoop will create the output folder (Results), so if you supply a folder that already exists Hadoop will fail).

18.	Repeat steps 14-17 with a different book. 


Shutting down the VM

i) Stop hadoop as follows: 

stop-yarn.sh
stop-dfs.sh

Wait for the hadoop processes to stop.


